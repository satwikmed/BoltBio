{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#Load the data\n",
        "def load_data(filepath):\n",
        "    # Load the data using chunking\n",
        "    chunks = pd.read_csv(filepath, chunksize=1000)  # Adjust chunk size as needed\n",
        "    data = pd.concat(chunks, ignore_index=True)\n",
        "    return data\n",
        "\n",
        "#Normalize the loaded data\n",
        "def normalize_data(data):\n",
        "    numeric_data = data.select_dtypes(include=[np.number])\n",
        "    total_counts_per_cell = numeric_data.sum(axis=1)\n",
        "    scaling_factor = total_counts_per_cell / 1e6\n",
        "    data_normalized = (numeric_data.div(scaling_factor, axis=0)) * 1e6\n",
        "    return data_normalized\n",
        "\n",
        "#Filepath by downloading it or through drive\n",
        "filepath = '/content/drive/MyDrive/Colab/File.csv'\n",
        "\n",
        "data = load_data(filepath)\n",
        "\n",
        "data_normalized = normalize_data(data)\n",
        "\n",
        "perplexity_value = 5 #Can be changed based on the dataset\n",
        "\n",
        "#Function to reduce dimensionality of the the large dataset using PCA and TSNE\n",
        "def reduce_dimension(data1, n_components):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_result = pca.fit_transform(data1.transpose())\n",
        "    tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity_value)\n",
        "    tsne_result = tsne.fit_transform(pca_result)\n",
        "    return tsne_result\n",
        "\n",
        "#Function to perform Clustering on the reduced dataset from above using K-Means\n",
        "def perform_clustering(data2, n_clusters, n_init):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=n_init).fit(data2)\n",
        "    return kmeans\n",
        "\n",
        "# Handle missing values using Imputation(Filling None values)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data_normalized), columns=data_normalized.columns, index=data_normalized.index)\n",
        "\n",
        "reduced_data = reduce_dimension(data_imputed, 6)\n",
        "\n",
        "kmeans_model = perform_clustering(reduced_data, 6, 10)\n",
        "cluster_labels = kmeans_model.labels_\n",
        "\n",
        "#Function to find the best two clusters from above to perform Differential Analysis on\n",
        "def select_top_clusters(cluster_labels, num_clusters=2):\n",
        "    cluster_counts = np.bincount(cluster_labels)\n",
        "    top_cluster_indices = np.argsort(cluster_counts)[-num_clusters:][::-1]\n",
        "    top_clusters = [i for i in top_cluster_indices if cluster_counts[i] > 0]\n",
        "    return top_clusters\n",
        "\n",
        "select_top_clusters(cluster_labels,2)\n",
        "\n",
        "#Differential Analysis is performed on the normalized data based on the cluster labels from above\n",
        "def differential_expression(data3, cluster_labels):\n",
        "    data3 = np.log1p(data3)\n",
        "    cluster0_cells = data3.columns[cluster_labels == 6]\n",
        "    cluster1_cells = data3.columns[cluster_labels == 4]\n",
        "    casedata = data3[cluster1_cells]\n",
        "    controldata = data3[cluster0_cells]\n",
        "    ttest_results = stats.ttest_ind(data3[cluster0_cells], data3[cluster1_cells], axis=1)\n",
        "    significant_genes = data3.index[ttest_results.pvalue < 0.05]\n",
        "    significant_gene_and_expression = data3.loc[significant_genes, :]\n",
        "    return significant_gene_and_expression\n",
        "\n",
        "deg_data = differential_expression(data_normalized, cluster_labels)\n",
        "\n",
        "#Visualizations\n",
        "#Clusters\n",
        "sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=kmeans_model.labels_, palette='viridis')\n",
        "plt.title('t-SNE Visualization of Clusters')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()\n",
        "\n",
        "#Differential Analysis data Box-Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=deg_data)\n",
        "plt.title(f'Expression of Significant Genes - Plot')\n",
        "plt.xlabel('Genes')\n",
        "plt.ylabel('Expression (Log-scale)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2V2_m5rVr8sK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}